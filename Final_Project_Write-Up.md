
### Scope and Gather Data

The purpose of this project was to create a data pipeline for a Mobile Gaming Studio that consumes player data, transforms it and loads it into reporting tables in Redshift. The goal of the system is to be cost-effective and scalable so that it can easily be adjusted to accommodate various number of users.

In this project I am using an established data schema that is generated by a load testing system that our company developed. The load testing script generates dummy data in a format that mimics real player data. Furthermore, the script is able to scale-up the number of users and data files to test the scalability of the data pipeline

### Data Sources

 This project uses two data sources:

1. **Game Player Data**: 
This is the main data file, formatted as a nested JSON, which contains player device, session and event information. The file contains state specific information, such as how much in-game currency the player has and also includes the action that trigger the event—appropriately named event.  
2. **Singular Data**: 
This is data file, formatted as CSV, that is sent by a third-party provider and outlines the cost of acquiring users for the mobile game. Since this is actual financial data, I have not included it in this project. However, you can view an example file which uses dummy data to get an idea of the data format.

While both of these sources are not dependent/related on each other, future downstream tables will use both of these sources (joined by source and country) to create tables that calculate ROI. 


#### Example of Game Player Data:


```python
{"states":{"device":{},"gamestate":{"user_name":"Andy","crowns":20,"league":"myleague","max_unlocked_court":"playground1","hard_currency":{"value":50},"soft_currency":{"value":100}},"session":{"user_id":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","session_id":"e00ebbcf-b227-4d9d-824f-4c83f597bd39","session_start_ts":1568922018768,"session_purchases":0,"session_value":0},"user":{"user_id":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","first_seen":1568915431069,"num_purchases":0,"customer_value":0,"player_level":20,"num_sessions":3,"ftue_complete":false}},"event":{"generalInfo":{"game":"victory","env":"test","platform":"Android","appVer":"bar","userId":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","installTime":"40","sessionId":"e00ebbcf-b227-4d9d-824f-4c83f597bd39","eventId":"52094234-e98c-4b36-a22a-154de476950a","eventTimestamp":"1568923220649","seqNum":"11","context":""},"playShot":{"court":"","ball":"","ballLevel":"0","pointZone":"0","cleanShot":false,"pointsScored":null,"currentScore":null,"opponentScore":null,"timeLeft":"0","timeSinceSpawn":"0","timeLeftRelease":"0","matchId":"eac914fb-978b-4827-b165-7e229d384e23","gyroscopeLevel":"0","tiebreak":"0","shotMade":false,"isBot":false,"matchType":"VICTORY_1v1","tournamentId":"","opponentId":"","ballX":0.85813725,"ballY":0.20664802,"touchStartX":0.35513717,"touchStartY":0.30587748,"touchEndX":0.23538259,"touchEndY":0.7942217,"touchDuration":"0","arcDrawPerc":0,"userActiveCards":[],"opponentActiveCards":[]}},"enriched":true,"process_timestamp":1568923242826,"process_filename":"events/2019/09/19/20/yeti-victory-test-1-2019-09-19-20-00-20-d569186b-a21a-434a-b892-757c6f5a84a5.gz","process_logstream":"2019/09/19/[$LATEST]6d84a16b60f24c31a9bd82faa1d9f278","process_origin":"file"}
```


```python
import pandas as pd

df = pd.read_csv('/Data_File_Examples/singular_2019-09-06 18_05_20.519281_0003_part_00.csv', header=None)

df.head(3)
```


<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8/10/19</td>
      <td>Victory</td>
      <td>Android</td>
      <td>AdWords</td>
      <td>P2_HC_GAD_GP_VN-th_EN_INST_2019-07-15</td>
      <td>VNM</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>3</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>P1Adwords</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.469870e+12</td>
      <td>NaN</td>
      <td>2019-09-06 18:05:36.888115+00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8/10/19</td>
      <td>Victory</td>
      <td>Android</td>
      <td>Organic</td>
      <td>NaN</td>
      <td>THA</td>
      <td>830.0</td>
      <td>1891.0</td>
      <td>2127.0</td>
      <td>1032.0</td>
      <td>...</td>
      <td>1078</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2019-09-06 18:05:36.888115+00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8/8/19</td>
      <td>Victory</td>
      <td>iOS</td>
      <td>Organic</td>
      <td>NaN</td>
      <td>USA</td>
      <td>1463.0</td>
      <td>1920.0</td>
      <td>1598.0</td>
      <td>1575.0</td>
      <td>...</td>
      <td>946</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2019-09-06 18:05:36.888115+00</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 21 columns</p>
</div>



As you can see from the data above, both sources of data are semi-structured with lots of Nulls in numerous fields.Due to this fact, the tables I've built in this project also allow for a flexible structure because we still want to include rows that are missing some column fields. 

The game player data load contained **1.2 million rows** while singular data is much smaller at **5k rows**. 

### Data Model

![Table_Model](https://user-images.githubusercontent.com/10493680/66167263-ddf44700-e5f6-11e9-8277-a32d771da25b.png)

The tables above reflect the initial schema of our tables. Each user event will have its own table to take advantage of Redshift's columnar storage. Furthermore, future tables will combine player data with singular data to calculate ROI for user acquisition campaigns.   

### Proposed Data Pipeline

![Tech_Stack](https://user-images.githubusercontent.com/10493680/66167396-4511fb80-e5f7-11e9-9626-9fe0c5948ddd.png)

The model above outlines the tools that are utilized by this pipeline
Since a major goal of this pipeline is scalability and cost savings, I have chosen tools that maximize those factors. 

Here are the detailed steps outlined in the model above:

1. Data is sent from the mobile client to our companie's S3 buckets via Amazon Firehose
2. The S3 bucket is linked to Amazon's SQS service which creates a message for everytime a file is placed into the bucket
3. These messages from SQS also serve as a trigger for our AWS lambda service which activates a Redshift Copy Command script once it is triggered. The number of messages that trigger a Lambda is a parameter that can be configured
4. Once the data is loaded into a staging table in Redshift, we use Apache Airflow to create event-specific tables that break out device and event-specific information. This tables are populated based on a Cron schedule.
5. Airflow also deletes data from the staging table on a daily cadence, such that the staging table only has a maximum of 30 days of data. This is done to reduce cost by keeping the Redshift cluster as small as possible
6. Finally, Amazon Athena is used to query raw data that is older that 30 days. Since this type of query is infrequent, we utilize a surverless architecture to query our raw files directly instead of scaling up our redshift cluster. 

### Other Scenarious 

Currently, the architecture allows us to load data based on the number of files that get PUT into the bucket. This not only allows us to get more recent data, but also allows us to copy data into redshift based on the data load from the client. The Lambda architecture also handles scale by managing concurrency, allowing us to focus on downstream parts of our pipeline

Furthermore, we can setup monitoring to make sure the lambda concurrency stays below a certain threshold. If Lambdas are exceeding this threshold, this would be an indicator that we need to increase the number of nodes on our redshift cluster to handle a higher volume of data loads.  

####  Data Increases 100x

If our data volume increases by such a large scale, we may need to adjust a few pieces of our pipeline. For example, the average data file for player events is around 5mg which easily fits into memory. If those files become too large to practically read or if the copy statements start exceeding the maximum run time of the lambda (currently 15 minutes), we would have to replace the lambda infrastructure with an EMR cluster that uses Spark for data loading and processing.

Since, we wouldn't store more than 30 days of data in our Redshift cluster, we wouldn't necessarily need to replace it with a different database or scale it up a large degree. However, we would need to rely on serverless architecture such as Athena or Spectrum, to query raw data which could prove to be expensive if those type of queries become more frequent.

#### Pipelines Would Run 7am Daily

If we need to run everything once per day, it's possible—depending  on data volume—that the copy command would run longer than the maximum run time of the lambda, which would require us to run the copy command on a EC2 machine on a cron schedule, instead of a Lambda.

#### Database Needs to be Accessed by 100 People

This would impact the type of Redshift cluster we run. To accommodate these many users, we would have to increase the number of nodes in the cluster. We would identify exactly how many nodes we need by looking at the workload manager concurrency stats. 
