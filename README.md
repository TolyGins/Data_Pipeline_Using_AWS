
## Project Description

The purpose of this project was to create a data pipeline for a Mobile Gaming Studio that consumes player data, transforms it and loads it into reporting tables in Redshift. The goal of the system is to be cost-effective and scalable so that it can easily be adjusted to accommodate various number of users.

In this project I am using an established data schema that is generated by a load testing system that our company developed. The load testing script generates dummy data in a format that mimics real player data. Furthermore, the script is able to scale-up the number of users and data files to test the scalability of the data pipeline

### Data Sources

 This project uses two data sources:

1. **Game Player Data**: 
This is the main data file, formatted as a nested JSON, which contains player device, session and event information. The file contains state specific information, such as how much in-game currency the player has and also includes the action that trigger the eventâ€”appropriately named event.  
2. **Singular Data**: 
This is data file, formatted as CSV, that is sent by a third-party provider and outlines the cost of acquiring users for the mobile game. Since this is actual financial data, I have not included it in this project. However, you can view an example file which uses dummy data to get an idea of the data format.

While both of these sources are not dependent/related on each other, future downstream tables will use both of these sources (joined by source and country) to create tables that calculate ROI. 

#### Example of Game Player Data:


```python
{"states":{"device":{},"gamestate":{"user_name":"Andy","crowns":20,"league":"myleague","max_unlocked_court":"playground1","hard_currency":{"value":50},"soft_currency":{"value":100}},"session":{"user_id":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","session_id":"e00ebbcf-b227-4d9d-824f-4c83f597bd39","session_start_ts":1568922018768,"session_purchases":0,"session_value":0},"user":{"user_id":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","first_seen":1568915431069,"num_purchases":0,"customer_value":0,"player_level":20,"num_sessions":3,"ftue_complete":false}},"event":{"generalInfo":{"game":"victory","env":"test","platform":"Android","appVer":"bar","userId":"1adbc9ff-a5ca-4409-8c66-75e21a181b25","installTime":"40","sessionId":"e00ebbcf-b227-4d9d-824f-4c83f597bd39","eventId":"52094234-e98c-4b36-a22a-154de476950a","eventTimestamp":"1568923220649","seqNum":"11","context":""},"playShot":{"court":"","ball":"","ballLevel":"0","pointZone":"0","cleanShot":false,"pointsScored":null,"currentScore":null,"opponentScore":null,"timeLeft":"0","timeSinceSpawn":"0","timeLeftRelease":"0","matchId":"eac914fb-978b-4827-b165-7e229d384e23","gyroscopeLevel":"0","tiebreak":"0","shotMade":false,"isBot":false,"matchType":"VICTORY_1v1","tournamentId":"","opponentId":"","ballX":0.85813725,"ballY":0.20664802,"touchStartX":0.35513717,"touchStartY":0.30587748,"touchEndX":0.23538259,"touchEndY":0.7942217,"touchDuration":"0","arcDrawPerc":0,"userActiveCards":[],"opponentActiveCards":[]}},"enriched":true,"process_timestamp":1568923242826,"process_filename":"events/2019/09/19/20/yeti-victory-test-1-2019-09-19-20-00-20-d569186b-a21a-434a-b892-757c6f5a84a5.gz","process_logstream":"2019/09/19/[$LATEST]6d84a16b60f24c31a9bd82faa1d9f278","process_origin":"file"}
```

### Data Pipeline

![Tech_Stack](https://user-images.githubusercontent.com/10493680/66167396-4511fb80-e5f7-11e9-9626-9fe0c5948ddd.png)


The model above outlines the tools that are utilized by this pipeline
Since a major goal of this pipeline is scalability and cost savings, I have chosen tools that maximize those factors. 

Here are the detailed steps outlined in the model above:

1. Data is sent from the mobile client to our companie's S3 buckets via Amazon Firehose
2. The S3 bucket is linked to Amazon's SQS service which creates a message for everytime a file is placed into the bucket
3. These messages from SQS also serve as a trigger for our AWS lambda service which activates a Redshift Copy Command script once it is triggered. The number of messages that trigger a Lambda is a parameter that can be configured
4. Once the data is loaded into a staging table in Redshift, we use Apache Airflow to create event-specific tables that break out device and event-specific information. This tables are populated based on a Cron schedule.
5. Airflow also deletes data from the staging table on a daily cadence, such that the staging table only has a maximum of 30 days of data. This is done to reduce cost by keeping the Redshift cluster as small as possible
6. Finally, Amazon Athena is used to query raw data that is older that 30 days. Since this type of query is infrequent, we utilize a surverless architecture to query our raw files directly instead of scaling up our redshift cluster. 

### Data Model

![Table_Model](https://user-images.githubusercontent.com/10493680/66167263-ddf44700-e5f6-11e9-8277-a32d771da25b.png)


### DAG

![DAG](https://user-images.githubusercontent.com/10493680/66167338-1431c680-e5f7-11e9-9a55-fd7a6a276b39.png)


### Running Project

1. Modify the dwh.cfg file with your db connection and credentials and S3 buckets where your data lives (you can use the example data files from this project). 
2. Add the json schema file to your desired S3 bucket
3. Run the etl.py file 

### Project Structure

1. **airflow_docker_setup** : This folder contains the environment to run the etl queries using the Airflow DAG.The folder is setup for easy deployment of Airflow to the cloud.
2. **images** : These are screenshots of the data model and Redshift Tables
3. **lambda_code** : This folder contains the zipped files and libraries to run the etl using AWS Lambda service. The main modification is changing the S3 bucket/file to be dynamic based on the event trigger 
4. **data_file_examples**: This contains example data files from our two sources of data
